\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{moreverb}
\usepackage{graphicx}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithm
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\title{Assignment 3 \\ EDA132 Applied Artificial Intelligence}
\date{\today}
\author{Fredrik Paulsson \\ dat11fp1@student.lu.se
\and Shan Senanayake \\ dat11sse@student.lu.se}
%\setcounter{secnumdepth}{5}
%\setcounter{tocdepth}{5}
\begin{document}
\maketitle
%\tableofcontents


% Report
% The assignment must be documented in a report, which should contain the following:

% The name of the author, the title of the assignment, and any relevant information on the front page.
% A presentation of the assignment.
% A presentation of the improvement(s) you have chosen.
% A presentation of your implementation and how to run the executable.
% A print-out of the example set(s) and the resulting decision tree(s).
% Comments on the results you have achieved.

\section{Introduction}
In this assignment we had to implement an algorithm that creates a decision tree based on examples from a dataset contained in Weka ARFF formatted files. This tree can then be used in supervised learning agents. One decision tree is created for one relation. A relation consists of a name, attributes that can take certain values and examples which state the classification for different combinations of values for the attributes.

\section{Improvements}
We have chosen to implement two of the three given improvements. Firstly we have implemented the pruning improvement because it seems as the most useful improvement to make. Secondly we have implemented the algorithm in such a way that it is possible to assign real values on the attributes instead of just enumerating different values allowed.

The Weka ARFF format specifies several different types of values that each attribute can have. In our solution we only allow two types of values. These are numeric values (real and integral) and nominal values. Nominal values are an enumeration of values that the attribute may take.

\section{Results}
We ran our algorithm for three different example sets, the example in the book, the weather.nominal and diabetes example set found in the Weka data folder. The example sets and their results are presented in Appendices \ref{18-3}, \ref{weather} and \ref{diabetes}.

\section{Discussion}
We have implemented pruning according to how the book explains it. However we are unsure if it works as it should do. For the small weather example it prunes even though we think that it might be wrong and for the big diabetes set it only prunes a few things where we think it should prune more. Since we don't have any example set to verify our pruning procedure we really don't know if it is correct.

We also ran into a strange problem when we during the pruning sent a \texttt{NaN} number in to the Chi Square distribution. We deduced that it were caused by the pruning algorithm trying to split combinations of values on the attributes that were not in example set which caused us to perform a zero by zero division. After this we believed that the pruning would work better but in practice it had only a very small effect.

The second improvement to represent numerical values in the tree seems to work by giving each attribute only two values for them to take. This means that each node in the tree gets two child nodes which represents values etiher bigger or smaller than the split point. For the example we had to set the split point somehow. At first we hade the same split point for all attributes but we later chose to set a split point for each attribute. For the diatebes example set the split points are set to the mean values for each attribute.


\section{Source Code}
Our implementation is split across several classes each defined in it's own .java file In this section we will briefly explain what the different classes do and highlight the most interesting parts of each class.

\subsection{Attribute.java}
This class represents an attribute with a set of values. The values are represented as \texttt{String}-objects, which simply is the names of the values. In the case of numerical values the attribute should only have two given values, defined by a splitpoint. There are a bunch of methodes in this class however the most interesting ones for this assignment are:
\begin{description}
\item[\texttt{public boolean test(String attributeValue,Example example)}] This method takes a \texttt{String attributeValue} which represents a value from this attribute, and an \texttt{Example example} which represents a example that should be tested. 
This method tests if the value is numerical and that it belongs to the right value branch defined by \texttt{attributeValue}. If it is not a numerical attribute then it just checks that the example is the current branch being evaluated.
\item[\texttt{public void setSplitPoint(double splitPoint)}] This method takes a \texttt{double splitPoint} which defines what value the two branches should be spilt by. This method is only called if it is a numerical attribute and then overrides all the previous values in the value set.
\item[\texttt{public String getKeyIfNumerical(String value)}] This method takes a \texttt{String value} which is a value from the set. If the attribute is numerical the correct \texttt{String}-object is returned. Otherwise the same value is returned. 
\end{description} 
\subsection{Goal.java}
This class can be seen as an extension to the class \texttt{Attribute} and it is used when classifying an example. It contains an attribute which is the attribute used for classification, for example, \texttt{willWait} which is used in the example set in the book. This class also takes a value which is assigned to the attribute and a boolean which indicates whether this was a positive or negative classification.
\subsection{Example.java}
This class represents an example that is fed to the decision tree learning algorithm. The class contains a map which has the attributes as keys and the values are each attribute's value. This class also contains a \texttt{Goal} which defines whether this example was classified as being negative or positive.
\subsection{Relation.java}
\texttt{Relation} is a class which represents an entire relation. Thus, it contains the name for the relation, a list of attributes and a list of examples.
\subsection{DecisionTreeParser.java}
This class is responsible for parsing a Weka ARFF formatted file in order to build a model with the classes we have defined above. Thus, it creates an object of the \texttt{Relation} class, which in turn consists of objects of \texttt{Attribute}, \texttt{Example}. This class does not really contain any interesting code but in order to solve the improvement to handle numerical values in the attributes it simply put \texttt{\%numeric} into the list of values allowed for the attribute. The parser contains some error handling when the file contains types that are not supported. In such cases it prints an error message and terminates execution.
\subsection{DecisionNode.java}
This class is a simple interface, used to store nodes in a tree of two different types. Namely \texttt{AttributeNode} and \texttt{TerminalNode}. The only use for this is to be able to represent two types of classes, with some shared objects in the same tree-datastructure.
\subsection{AttributeNode.java}
This class represents a branch in the decision tree. This class consists of an \texttt{Attribute} and a list of \texttt{Example}-objects. The \texttt{Attribute} of the node determines which attribute this branch is, and the list of \texttt{Example}-objects are the examples that are left when we have reached this attribute in the decision tree algorithm. 
\subsection{TerminalNode.java}
This class represents a leaf in the decision tree, which only consists of a \texttt{Goal} - value. 
\subsection{DecisionTreeAlgorithm.java}
This class represents the main algorithm in building and pruning the decision tree. This class takes a \texttt{Relation} and creates a decision tree.
\begin{description}
\item[\texttt{public DecisionNode dtl()}] This method creates a decision tree from the relation given to the constructor. This method uses several private methods to build the tree. The most important of them are the \texttt{private DecisionNode decisionTreeAlgorithm(DecisionNode node)} which represents the recursive method doing the building of the tree and \texttt{private Attribute mostImporatant(ArrayList<Attribute> attributes, ArrayList<Example> examples)} which reprsents the importance function that are implemented just like the book. 
\item[\texttt{public DecisionNode pruning(DecisionNode tree) }] This method takes a decision tree and prunes it using the standard deviation algorithm explained in the book for pruning. Just like the building of the tree this method uses different types of private methods to calculate the deviation. This method could have been static since it is only depdent on the input. 
\end{description}
\subsection{DecTreeMain.java}
This class consists of the main method of the program. This starts the program and initiates all the necessary classes and runs the decision tree algorithm for three different files.

\section{Library}
We have chosen to use an external library to help get the $\chi^{2}$ table needed to do a $\chi^{2}$-pruning. The library is called \emph{SSJ: Stochastic Simulation in Java} and is developed by Pierre L'Ecuyer. It is available at \texttt{http://simul.iro.umontreal.ca/ssj/indexe.html}.


\section{Running Instructions} The .java files containing the classes described
above are located in a package called decision\_tree placed on
/h/d9/v/dat11fp1/TAI/decision\_trees on the student computer system. It
is simply to compile the java files in the package and then run DecTreeMain from
outside that package. However, in order to compile the program the \texttt{ssj.jar} file needs to be included. For example if the current directory is
/h/d9/v/dat11fp1/TAI the following command will compile the files: \texttt{javac -cp .:ssj.jar
decision\_trees/*.java}. After compiling the program can be run with
the following command: \texttt{java -cp .:ssj.jar decision\_trees.DecTreeMain}. The program requires three arff files in order to run but these have been placed on the student computer system as well. The program will run according to the above instructions.

%\begin{thebibliography}{1}
%\bibitem{wikipedia}
%http://en.wikipedia.org
%\end{thebibliography}
\appendix
\section{Example 18-3}
\label{18-3}
The example set:
\verbatiminput{arff18-3.txt}
The results:
\verbatiminput{result-18-3.txt}
\section{weather.nominal}
\label{weather}
The example set:
\verbatiminput{weather.nominal.arff}
The results:
\verbatiminput{result-weather.txt}

\section{diabetes}
\label{diabetes}
The example set:
\verbatiminput{diabetes.arff}
The results:
\verbatiminput{result-diabetes.txt}

\end{document}
