\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{moreverb}
\usepackage{graphicx}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithm
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\title{Assignment 3 \\ EDA132 Applied Artificial Intelligence}
\date{\today}
\author{Fredrik Paulsson \\ dat11fp1@student.lu.se
\and Shan Senanayake \\ dat11sse@student.lu.se}
%\setcounter{secnumdepth}{5}
%\setcounter{tocdepth}{5}
\begin{document}
\maketitle
%\tableofcontents


% Report
% The assignment must be documented in a report, which should contain the following:

% The name of the author, the title of the assignment, and any relevant information on the front page.
% A presentation of the assignment.
% A presentation of the improvement(s) you have chosen.
% A presentation of your implementation and how to run the executable.
% A print-out of the example set(s) and the resulting decision tree(s).
% Comments on the results you have achieved.

\section{Introduction}
In this assignment we had to implement an algorithm that creates a decision tree based on some examples. This tree can then be used in supervised learning agents. One decision tree is created for one relation. A relation consists of a name, attributes that can take certain values and examples which state the classification for different combinations of values for the attributes.

\section{Improvements}
We have chosen to implement two of the three given improvements. Firstly we have implemented the pruning improvement because it seems as the most useful improvement to make. Secondly we have implemented the algorithm in such a way that it is possible to assign real values on the attributes instead of just enumerating different values allowed.

The Weka ARFF format specifies several different types of values that each attribute can have. In our solution we only allow two types of values. These are numeric values (real and integral) and nominal values. Nominal values are an enumeration of values that the attribute may take.

\section{Results}
We ran our algorithm for three different example sets, the example in the book, the weather.nominal and diabetes example set found in the Weka data folder. The results are presented in Appendices \ref{18-3}, \ref{weather} and \ref{diabetes}.

\section{Discussion}

\section{Source Code}
Our implementation is split across several classes each defined in it's own .java file In this section we will briefly explain what the different classes do and highlight the most interesting parts of each class.

\subsection{Attribute.java}
This class represents an attribute with a set of values. The values are represented as \texttt{String}-objects, which simply is the names of the values. In the case of numerical values the attribute should only have two given values, defined by a splitpoint. There are a bunch of methodes in this class however the most interesting ones for this assignment are:
\begin{description}
\item[\texttt{public boolean test(String attributeValue,Example example)}] This method takes a \texttt{String attributeValue} which represents a value from this attribute, and an \texttt{Example example} which represents a example that should be tested. 
This method tests if the value is numerical and that it belongs to the right value branch defined by \texttt{attributeValue}. If it is not a numerical attribute then it just checks that the example is the current branch being evaluated.
\item[\texttt{public void setSplitPoint(double splitPoint)}] This method takes a \texttt{double splitPoint} which defines what value the two branches should be spilt by. This method is only called if it is a numerical attribute and then overrides all the previous values in the value set.
\item[\texttt{public String getKeyIfNumerical(String value)}] This method takes a \texttt{String value} which is a value from the set. If the attribute is numerical the correct \texttt{String}-object is returned. Otherwise the same value is returned. 
\end{description} 
\subsection{Goal.java}
This class can be seen as an extension to the class \texttt{Attribute} and it is used when classifying an example. It contains an attribute which is the attribute used for classification, for example, \texttt{willWait} which is used in the example set in the book. This class also takes a value which is assigned to the attribute and a boolean which indicates whether this was a positive or negative classification.
\subsection{Example.java}
This class represents an example that is fed to the decision tree learning algorithm. The class contains a map which has the attributes as keys and the values are each attribute's value. This class also contains a \texttt{Goal} which defines whether this example was classified as being negative or positive.
\subsection{Relation.java}
\texttt{Relation} is a class which represents an entire relation. Thus, it contains the name for the relation, a list of attributes and a list of examples.
\subsection{DecisionTreeParser.java}
This class is responsible for parsing a Weka ARFF formatted file in order to build a model with the classes we have defined above. Thus, it creates an object of the \texttt{Relation} class, which in turn consists of objects of \texttt{Attribute}, \texttt{Example}. This class does not really contain any interesting code but in order to solve the improvement to handle numerical values in the attributes it simply put \texttt{\%numeric} into the list of values allowed for the attribute. The parser contains some error handling when the file contains types that are not supported. In such cases it prints an error message and terminates execution.
\subsection{DecisionNode.java}
This class is a simple interface, used to store nodes in a tree of two different types. Namely \texttt{AttributeNode} and \texttt{TerminalNode}. The only use for this is to be able to represent two types of classes, with some shared objects in the same tree-datastructure.
\subsection{AttributeNode.java}
This class represents a branch in the decision tree. This class consists of an \texttt{Attribute} and a list of \texttt{Example}-objects. The \texttt{Attribute} of the node determines which attribute this branch is, and the list of \texttt{Example}-objects are the examples that are left when we have reached this attribute in the decision tree algorithm. 
\subsection{TerminalNode.java}
This class represents a leaf in the decision tree, which only consists of a \texttt{Goal} - value. 
\subsection{DecisionTreeAlgorithm.java}
This class represents the main algorithm in building and pruning the decision tree. This class takes a \texttt{Relation} and creates a decision tree.
\item[\texttt{public DecisionNode dtl()}] This method creates a decision tree from the relation given to the constructor. This method uses several private methods to build the tree. The most important of them are the \texttt{private DecisionNode decisionTreeAlgorithm(DecisionNode node)} which represents the recursive method doing the building of the tree and \texttt{private Attribute mostImporatant(ArrayList<Attribute> attributes, ArrayList<Example> examples)} which reprsents the importance function that are implemented just like the book. 
\item[\texttt{public DecisionNode pruning(DecisionNode tree) }] This method takes a decision tree and prunes it using the standard deviation algorithm explained in the book for pruning. Just like the building of the tree this method uses different types of private methods to calculate the deviation. This method could have been static since it is only depdent on the input. 
\subsection{DecTreeMain}
This class consists of the main method of the program. This starts the program and initiates all the necessary classes and runs the decision tree algorithm for three different files.

\section{Library}
We have chosen to use an external library to help get the $\chi^{2}$ table needed to do a $\chi^{2}$-pruning. The library is called \emph{SSJ: Stochastic Simulation in Java} and is developed by Pierre L'Ecuyer. It is available at \texttt{http://simul.iro.umontreal.ca/ssj/indexe.html}.


\section{Running Instructions} The .java files containing the classes described
above are located in a package called decision\_tree placed on
/h/d9/v/dat11fp1/TAI/decision\_tree on the student computer system. It
is simply to compile the java files in the package and then run DecTreeMain from
outside that package. However, in order to compile the program the \texttt{ssj.jar} file needs to be included. For example if the current directory is
/h/d9/v/dat11fp1/TAI the following command will compile the files: \texttt{javac -cp "ssj.jar;"
decision\_tree/*.java}. After compiling the program can be run with
the following command: \texttt{java decision\_tree.DecTreeMain}. The program requires three arff files in order to run but these have been placed on the student computer system as well. The program will run according to the above instructions.

%\begin{thebibliography}{1}
%\bibitem{wikipedia}
%http://en.wikipedia.org
%\end{thebibliography}
\appendix
\section{Example 18-3}
\label{18-3}
\verbatiminput{result-18-3.txt}
\section{weather.nominal}
\label{weather}
\verbatiminput{result-weather.txt}

\section{diabetes}
\label{diabetes}
\verbatiminput{result-diabetes.txt}

\end{document}
